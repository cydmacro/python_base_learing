# 典型案例展示

> 本文档展示AI训练师岗位的3个真实工作场景,帮助学员理解所学技能的实际应用

---

## 案例1: 电商商品图片数据标注项目

### 项目背景

**客户**: 某大型电商平台
**需求**: 训练商品图片分类AI模型,需要标注50000张商品图片
**类别**: 服装、电子产品、食品、家居用品、美妆、母婴(6个类别)
**时间**: 30天完成

### 技术要求

**用到的Python技能**:
- Pandas读取CSV格式的标注任务列表
- 批量处理图片文件名(重命名/分类)
- 数据质量检查(缺失值/重复值)
- 生成标注进度报告

**用到的工具**:
- Label Studio(标注工具)
- Python脚本(自动化处理)
- Excel(质检报告)

### 工作流程

**第1周:数据准备**
```python
# 1. 批量重命名图片
import os

for i, filename in enumerate(os.listdir('raw_images')):
    new_name = f'product_{i+1:05d}.jpg'
    os.rename(f'raw_images/{filename}', f'processed/{new_name}')

# 2. 生成标注任务清单
import pandas as pd

files = os.listdir('processed')
df = pd.DataFrame({'文件名': files, '标签': ''})
df.to_csv('annotation_tasks.csv', index=False)
```

**第2-4周:标注与质检**
- 10名标注员并行标注
- 每人每天完成200张
- 2名质检员抽检20%数据
- 使用Python脚本统计进度

```python
# 每日进度统计脚本
import pandas as pd

df = pd.read_csv('labels.csv')

# 统计每个标注员的工作量
work_stats = df['标注员'].value_counts()
print("今日工作量:")
print(work_stats)

# 统计总进度
total = 50000
done = len(df[df['标签'].notna()])
progress = done / total * 100
print(f"\n总进度: {progress:.1f}% ({done}/{total})")
```

**第5周:数据清洗与导出**
```python
import pandas as pd

# 读取标注数据
df = pd.read_csv('raw_labels.csv')

# 数据清洗
df = df.dropna(subset=['标签'])  # 删除未标注
df = df.drop_duplicates(subset=['文件名'])  # 去重
df = df[df['标签'].isin(['服装','电子','食品','家居','美妆','母婴'])]  # 只保留有效标签

# 检查数据均衡性
label_dist = df['标签'].value_counts()
print(label_dist)

# 导出训练数据
df[['文件名', '标签']].to_csv('train_dataset.csv', index=False)
```

### 项目成果

- ✅ 按时完成50000张图片标注
- ✅ 标注准确率98.5%(质检抽样)
- ✅ 数据分布均衡(每个类别8000±500张)
- ✅ 客户满意度:五星好评

### 学员能做的工作

通过我的10天课程,学员可以胜任:
- **标注员**: 使用Label Studio标注图片
- **质检员**: 检查标注质量,统计错误率
- **数据处理**: 用Python脚本清洗数据
- **进度统计**: 自动生成工作量报告

---

## 案例2: 客服对话数据清洗与标注

### 项目背景

**客户**: 某在线教育平台
**需求**: 训练智能客服对话AI,需要处理100000条历史对话记录
**任务**: 清洗数据 + 情感标注(正面/负面/中性)
**时间**: 20天完成

### 技术要求

**用到的Python技能**:
- 文本清洗(正则表达式)
- Pandas数据处理
- 数据筛选(条件过滤)
- 质量分析报告生成

### 数据问题

原始对话数据存在的问题:
- 包含大量无效字符(表情符号、HTML标签)
- 存在重复对话
- 部分对话为空或无意义("哦"、"嗯")
- 格式不统一(有的有时间戳,有的没有)

### 解决方案

**步骤1: 文本清洗**
```python
import pandas as pd
import re

def clean_text(text):
    """清洗单条对话"""
    if pd.isna(text):
        return ''

    # 去除HTML标签
    text = re.sub(r'<[^>]+>', '', text)

    # 去除表情符号和特殊字符
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9,。!?、 ]', '', text)

    # 去除多余空格
    text = ' '.join(text.split())

    return text.strip()

# 读取数据
df = pd.read_csv('raw_dialogs.csv', encoding='utf-8')

# 应用清洗
df['对话'] = df['对话'].apply(clean_text)

print(f"原始数据: {len(df)}条")

# 删除无效对话
df = df[df['对话'].str.len() >= 5]  # 至少5个字符
df = df[~df['对话'].isin(['哦', '嗯', '好的', '谢谢'])]  # 删除无意义对话

print(f"清洗后: {len(df)}条")
```

**步骤2: 去重**
```python
# 基于对话内容去重
df = df.drop_duplicates(subset=['对话'])

print(f"去重后: {len(df)}条")
```

**步骤3: 情感标注**

使用Label Studio标注情感:
- 正面: "你们的服务太棒了!"
- 负面: "等了半天没人回复,很失望"
- 中性: "请问退款流程是什么?"

**步骤4: 质量检查**
```python
# 读取标注数据
df_labeled = pd.read_csv('labeled_dialogs.csv')

# 检查标签分布
label_dist = df_labeled['情感'].value_counts()
print("情感分布:")
print(label_dist)

# 检查数据质量
missing = df_labeled['情感'].isnull().sum()
print(f"\n缺失标签: {missing}条")

# 抽样检查
sample = df_labeled.sample(100, random_state=42)
sample.to_csv('quality_check_sample.csv', index=False)
print("✅ 已生成质检样本")
```

### 项目成果

- ✅ 100000条对话清洗完成
- ✅ 保留有效对话75000条
- ✅ 标注准确率96%(人工抽检1000条)
- ✅ 数据格式标准化,可直接用于训练

### 学员能做的工作

- **数据清洗**: 用正则表达式处理文本
- **质量检查**: 统计缺失值、重复值
- **标注任务**: 使用Label Studio标注情感
- **数据导出**: 生成标准格式训练集

---

## 案例3: 模型训练数据集准备完整流程

### 项目背景

**客户**: AI算法公司(内部项目)
**需求**: 准备图像分类模型训练数据集
**数据来源**: 多个标注团队提供的CSV文件
**任务**: 整合、清洗、划分数据集
**时间**: 5天完成

### 技术要求

**用到的Python技能**:
- 批量读取多个CSV文件
- Pandas数据合并(concat)
- 数据清洗完整流程
- 训练集/测试集划分
- 数据质量报告生成

### 工作流程

**Day1: 数据整合**
```python
import pandas as pd
import os

# 读取多个CSV文件
csv_files = ['team_a.csv', 'team_b.csv', 'team_c.csv']

dfs = []
for file in csv_files:
    df = pd.read_csv(file, encoding='utf-8')
    df['来源'] = file.split('.')[0]  # 记录数据来源
    dfs.append(df)

# 合并数据
df_all = pd.concat(dfs, ignore_index=True)

print(f"合并后总数据: {len(df_all)}条")
print(f"各团队数据量:")
print(df_all['来源'].value_counts())
```

**Day2: 数据清洗**
```python
# 清洗流程
print("=" * 60)
print("数据清洗流程")
print("=" * 60)

original_count = len(df_all)

# 1. 删除标签缺失
df_clean = df_all.dropna(subset=['标签'])
print(f"步骤1 - 删除标签缺失: {len(df_all)} → {len(df_clean)}")

# 2. 删除无效文件格式
df_clean = df_clean[df_clean['文件名'].str.endswith(('.jpg', '.png'))]
print(f"步骤2 - 删除无效格式: {len(df_clean)}条")

# 3. 去重(保留最新标注)
df_clean = df_clean.sort_values('标注时间', ascending=False)
df_clean = df_clean.drop_duplicates(subset=['文件名'], keep='first')
print(f"步骤3 - 去重: {len(df_clean)}条")

# 4. 只保留有效标签
valid_labels = ['猫', '狗', '鸟']
df_clean = df_clean[df_clean['标签'].isin(valid_labels)]
print(f"步骤4 - 过滤标签: {len(df_clean)}条")

# 5. 只保留质检通过
df_clean = df_clean[df_clean['质检状态'] == '通过']
print(f"步骤5 - 质检通过: {len(df_clean)}条")

print(f"\n保留率: {len(df_clean)/original_count*100:.1f}%")
```

**Day3: 数据质量分析**
```python
# 生成详细报告
print("\n" + "=" * 60)
print("数据质量报告")
print("=" * 60)

# 标签分布
label_dist = df_clean['标签'].value_counts()
print("\n标签分布:")
for label, count in label_dist.items():
    percentage = count / len(df_clean) * 100
    print(f"  {label}: {count}条 ({percentage:.1f}%)")

# 来源分布
source_dist = df_clean['来源'].value_counts()
print("\n各团队贡献:")
for source, count in source_dist.items():
    percentage = count / len(df_clean) * 100
    print(f"  {source}: {count}条 ({percentage:.1f}%)")

# 数据均衡性检查
max_count = label_dist.max()
min_count = label_dist.min()
balance_ratio = min_count / max_count * 100

print(f"\n数据均衡度: {balance_ratio:.1f}%")
if balance_ratio < 70:
    print("⚠️ 警告: 数据不均衡!")
else:
    print("✅ 数据分布较均衡")
```

**Day4: 数据集划分**
```python
from sklearn.model_selection import train_test_split

# 划分训练集(80%)和测试集(20%)
train_df, test_df = train_test_split(
    df_clean,
    test_size=0.2,
    stratify=df_clean['标签'],  # 分层抽样
    random_state=42
)

print(f"\n训练集: {len(train_df)}条")
print(train_df['标签'].value_counts())

print(f"\n测试集: {len(test_df)}条")
print(test_df['标签'].value_counts())

# 保存
train_df[['文件名', '标签']].to_csv('train.csv', index=False, encoding='utf-8-sig')
test_df[['文件名', '标签']].to_csv('test.csv', index=False, encoding='utf-8-sig')

print("\n✅ 数据集已保存: train.csv, test.csv")
```

**Day5: 生成文档**
```python
# 生成README.md
readme = f"""
# 图像分类数据集

## 数据集信息
- 创建日期: 2024-01-15
- 数据来源: 3个标注团队
- 总样本数: {len(df_clean)}条
- 训练集: {len(train_df)}条
- 测试集: {len(test_df)}条

## 类别分布
{label_dist}

## 数据质量
- 标注准确率: 98%+ (质检通过)
- 数据均衡度: {balance_ratio:.1f}%
- 图片格式: JPG/PNG

## 使用方法
```python
import pandas as pd

# 读取训练集
train_df = pd.read_csv('train.csv')
```

"""

with open('README.md', 'w', encoding='utf-8') as f:
    f.write(readme)

print("✅ README.md已生成")
```

### 项目成果

- ✅ 整合3个团队数据,总计12000条
- ✅ 清洗后保留8500条高质量数据
- ✅ 数据均衡度85%(接近理想状态)
- ✅ 提供完整文档和使用说明

### 学员能做的工作

这个案例涵盖了AI数据处理工程师的核心工作:
- ✅ 数据整合(多源数据合并)
- ✅ 数据清洗(完整流程)
- ✅ 质量分析(生成报告)
- ✅ 数据集划分(train/test split)
- ✅ 文档编写(README)

**这就是我们10天课程要培养的能力!**

---

## 案例总结

### 三个案例的共同点

| 核心技能 | 案例1 | 案例2 | 案例3 |
|---------|-------|-------|-------|
| Pandas读写CSV | ✅ | ✅ | ✅ |
| 数据清洗 | ✅ | ✅ | ✅ |
| 数据筛选 | ✅ | ✅ | ✅ |
| 统计分析 | ✅ | ✅ | ✅ |
| 质量检查 | ✅ | ✅ | ✅ |
| 报告生成 | ✅ | ✅ | ✅ |

### 对应的课程章节

| 技能点 | 课程章节 |
|--------|---------|
| Python基础语法 | Day1-2 |
| 循环与条件判断 | Day3-4 |
| 函数封装 | Day5 |
| 文件操作 | Day6 |
| Pandas数据处理 | Day7 |
| 标注工具使用 | Day8 |
| 项目实战 | Day9-10 |

---

**这些案例证明: 我们的10天课程内容100%对接真实工作场景!** 🎯

```
